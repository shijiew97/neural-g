% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/neural_g2.R
\name{neural_g2}
\alias{neural_g2}
\title{Bivariate neural-g}
\usage{
neural_g2(X=NULL, Y, param=1, dist=NULL, hidden_size=500,
num_it=NULL, p=1, gpu_ind=0, L=5, n_grid=100,
lr=0.0001, n0=n, n=NULL, lrDecay=0, lrpower=0.2,
verb=1)
}
\arguments{
\item{X}{predictor Variable.}

\item{Y}{response Variable.}

\item{param}{nuisance parameter value. param = 0 indicates there is no nuisance parameter.}

\item{dist}{probabilistic distribution of Y, including:
"Gaussian2", "Gaussian2s".}

\item{hidden_size}{number of hidden neurons at each layer.}

\item{num_it}{number of iterations for training.}

\item{p}{the dimension of predictor Variable;should be > 1.}

\item{c}{the weight parameter in weighted average gradients;Default is 0.5.}

\item{gpu_ind}{gpu index.}

\item{L}{number of hidden layers.}

\item{n_grid}{number of support grids.}

\item{n0}{number of mini-batch size;Default is equal to n.}

\item{n}{number of sample size.}

\item{verb}{print information while training generator.}

\item{lr0}{learning rate, default is 0.0001.}

\item{lrDecay}{lrDecay = 1: using decaying learning rate.}

\item{lrpower}{decay rate of learning rate, default is 0.2.}
}
\value{
neural_g2 function returns a list with support grids (''support'') and estimated probability mass (''prob'')
}
\description{
Train Bivariate neural-g.
python(>=3.7) and pytorch
are needed to be installed in advance. R pakcage 'reticulate' is also required.
}
\examples{
### Gasussian discrete
Seed <- 128783;set.seed(Seed);dist <- "Gaussian2";param <- 0.5
n <- 1000;L <- 5;num_it <- 4000;n_grid <- 50;p <- 2
theta<-c(0, 2);mix_prob<-c(0.2, 0.8);theta1<-rep(0,n);theta2<-rep(0,n)
Y <- matrix(0, n, p)
for(i in 1:n){
    mu = base::sample(theta, size=1, prob=mix_prob)
    sigma = 1*(mu==0)+0.1*(mu==2)
    Y[i,] = mu+sigma*rnorm(p)
    theta1[i] = mu;theta2[i] = sigma
}
net_g <- neural_g2(Y=Y, param=param, dist=dist, n=n, num_it=num_it, n_grid=n_grid, p=p)
mu_support <- unique(net_g$support[,1])
sig_support <- unique(net_g$support[,2])
mu_prob <- rep(0, n_grid);sigma_prob <- rep(0, n_grid)
for(i in 1:n_grid){
    mu_prob[i] <- sum( net_g$prob[net_g$support[,1]==mu_support[i]] )
    sigma_prob[i] <- sum( net_g$prob[net_g$support[,2]==sig_support[i]] )
 }
plot(mu_support, mu_prob, col=rgb(1,0,0,0.8), type='l', xlim=c(-1,3),
     xlab='support', ylab='mass', lwd=3, ylim=c(0,1.2), cex.axis=1.85, cex.lab=1.85)
lines(sig_support, sigma_prob, col=rgb(0,0,1,0.8), type='l', lwd=3, lty=2)
legend(x=2.0, y=1.3, c(expression(mu),expression(sigma^2)),
       col=c(rgb(1,0,0),rgb(0,0,1)), lty=c(1,2), lwd=c(3,3),
       bty="n", x.intersp=0.5, y.intersp=0.8, seg.len=1.0, cex=1.85)
### Gasussian smooth
Seed <- 128783;set.seed(Seed);dist <- "Gaussian2s";param <- 0.5
n <- 1000;L <- 5;num_it <- 4000;n_grid <- 50;p <- 2
mu <- 1;sigma <- 1; shape <- 2;scale <- 0.5
Y <- matrix(0, n, p)
theta1 <- MCMCpack::rinvgamma(n, shape, scale)
theta2 <- rnorm(n, mu, sd = sqrt(theta1 * sigma^2))
for(i in 1:n){Y[i,] <- theta2[i]+theta1[i]*rnorm(p)}
net_g <- neural_g2(Y=Y, param=param, dist=dist, n=n, num_it=num_it, n_grid=n_grid, p=p)
mu_support <- unique(net_g$support[,1])
sig_support <- unique(net_g$support[,2])
mu_prob <- rep(0, n_grid);sigma_prob <- rep(0, n_grid)
for(i in 1:n_grid){
    mu_prob[i] <- sum( net_g$prob[net_g$support[,1]==mu_support[i]] )
    sigma_prob[i] <- sum( net_g$prob[net_g$support[,2]==sig_support[i]] )
 }
plot(mu_support, mu_prob, col=rgb(1,0,0,0.8), type='l', xlim=c(-3,4),
     xlab='support', ylab='mass', lwd=3, ylim=c(0,0.35), cex.axis=1.85, cex.lab=1.85)
lines(sig_support, sigma_prob, col=rgb(0,0,1,0.8), type='l', lwd=3, lty=2)
legend(x=2.0, y=0.385, c(expression(mu),expression(sigma^2)),
       col=c(rgb(1,0,0),rgb(0,0,1)), lty=c(1,2), lwd=c(3,3),
       bty="n", x.intersp=0.5, y.intersp=0.8, seg.len=1.0, cex=1.85)
}
\author{
Shijie Wang, Chakraborty Saptarshi, Qin Qian, Ray Bai
}
